{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+Xop2ATJMd7lUP9aWudtZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/like-as-though-not-a-benchmark/blob/main/LoRA_(PEFT_QLoRA)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A) LoRA (PEFT/QLoRA) for CST category classification (sentence-level)**"
      ],
      "metadata": {
        "id": "X5muBJEbQ9D3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4ZUCHnCQdqy"
      },
      "outputs": [],
      "source": [
        "!pip -q install \"transformers>=4.44.0\" \"datasets>=2.20.0\" \"peft>=0.13.0\" \"accelerate>=0.33.0\" \"bitsandbytes>=0.43.1\" evaluate -U\n",
        "\n",
        "import os, json, ast, math, re, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
        "                          Trainer, TrainingArguments, set_seed)\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import evaluate\n",
        "\n",
        "ROOT = Path(\"/mnt/data/cst_v2_joyce_similes\") if Path(\"/mnt/data\").exists() else Path(\"/content/cst_v2_joyce_similes\")\n",
        "PROC = ROOT / \"data\" / \"processed\"\n",
        "OUT  = ROOT / \"outputs\"\n",
        "MODELS = ROOT / \"models\"\n",
        "OUT.mkdir(parents=True, exist_ok=True); MODELS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# processed input\n",
        "CSV_PATH = PROC / \"merged_all_processed.csv\"\n",
        "assert CSV_PATH.exists(), f\"Missing {CSV_PATH}\"\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "df[\"sentence\"] = df[\"sentence\"].astype(str).str.strip()\n",
        "\n",
        "label_col = None\n",
        "for c in [\"gold_category\", \"category\", \"extractor_label\"]:\n",
        "    if c in df.columns and df[c].dropna().astype(str).nunique() >= 2:\n",
        "        label_col = c; break\n",
        "assert label_col is not None, \"No viable label column (need ≥2 classes in gold_category/category/extractor_label).\"\n",
        "\n",
        "if \"story\" not in df.columns:\n",
        "    if \"source_file\" in df.columns:\n",
        "        df[\"story\"] = df[\"source_file\"].astype(str)\n",
        "    else:\n",
        "        df[\"story\"] = \"ALL\"\n",
        "\n",
        "# label ids\n",
        "df[label_col] = df[label_col].astype(str)\n",
        "labels = sorted(df[label_col].unique())\n",
        "label2id = {l:i for i,l in enumerate(labels)}\n",
        "id2label = {i:l for l,i in label2id.items()}\n",
        "df[\"label_id\"] = df[label_col].map(label2id)\n",
        "\n",
        "# Train/val split with grouped stratification\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, val_idx = next(gss.split(df, groups=df[\"story\"]))\n",
        "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
        "val_df   = df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "# Tok\n",
        "MODEL_NAME = \"roberta-base\"           # can swap to microsoft/deberta-v3-base\n",
        "MAX_LEN    = 256\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "def to_hf(ds):\n",
        "    return Dataset.from_pandas(ds[[\"sentence\",\"label_id\"]].rename(columns={\"sentence\":\"text\",\"label_id\":\"label\"}))\n",
        "\n",
        "train_ds = to_hf(train_df)\n",
        "val_ds   = to_hf(val_df)\n",
        "raw = DatasetDict({\"train\": train_ds, \"validation\": val_ds})\n",
        "\n",
        "def tok_fn(batch):\n",
        "    enc = tok(batch[\"text\"], max_length=MAX_LEN, truncation=True, padding=\"max_length\")\n",
        "    enc[\"labels\"] = batch[\"label\"]\n",
        "    return enc\n",
        "\n",
        "tokd = raw.map(tok_fn, batched=True, remove_columns=raw[\"train\"].column_names)\n",
        "\n",
        "# Metrics\n",
        "acc = evaluate.load(\"accuracy\")\n",
        "f1  = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    return {\n",
        "        \"accuracy\": acc.compute(predictions=preds, references=p.label_ids)[\"accuracy\"],\n",
        "        \"macro_f1\": f1.compute(predictions=preds, references=p.label_ids, average=\"macro\")[\"f1\"]\n",
        "    }\n",
        "\n",
        "# call my daughter LoRA\n",
        "lora_cfg = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "    target_modules=[\"query\",\"value\",\"key\",\"q_proj\",\"v_proj\",\"k_proj\"]  # robust across RoBERTa/DeBERTa variants\n",
        ")\n",
        "\n",
        "# b model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(labels), id2label=id2label, label2id=label2id)\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "# Optional: focal loss toggle\n",
        "USE_FOCAL = True\n",
        "GAMMA = 2.0\n",
        "\n",
        "import torch.nn as nn\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        self.ce = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "    def forward(self, logits, targets):\n",
        "        ce = self.ce(logits, targets)\n",
        "        pt = torch.softmax(logits, dim=-1).gather(1, targets.unsqueeze(1)).squeeze(1).clamp_min(1e-6)\n",
        "        fl = (1 - pt) ** self.gamma * ce\n",
        "        return fl.mean() if self.reduction==\"mean\" else fl.sum()\n",
        "\n",
        "# Trainer args (QLoRA/4-bit works if GPU available)\n",
        "out_dir = str(MODELS / \"roberta_cst_lora\")\n",
        "args = TrainingArguments(\n",
        "    output_dir=out_dir,\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\",\n",
        "    greater_is_better=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    bf16=False\n",
        ")\n",
        "\n",
        "class FocalTrainer(Trainer):\n",
        "    def __init__(self, *args, use_focal=False, gamma=2.0, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.use_focal = use_focal\n",
        "        self.focal = FocalLoss(gamma=gamma)\n",
        "        self.ce = nn.CrossEntropyLoss()\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss = self.focal(logits, labels) if self.use_focal else self.ce(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "trainer = FocalTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokd[\"train\"],\n",
        "    eval_dataset=tokd[\"validation\"],\n",
        "    tokenizer=tok,\n",
        "    compute_metrics=compute_metrics,\n",
        "    use_focal=USE_FOCAL, gamma=GAMMA\n",
        ")\n",
        "\n",
        "set_seed(42)\n",
        "trainer.train()\n",
        "\n",
        "# Save\n",
        "trainer.save_model(out_dir)\n",
        "tok.save_pretrained(out_dir)\n",
        "(Path(out_dir) / \"label2id.json\").write_text(json.dumps(label2id, indent=2))\n",
        "print(\"Saved:\", out_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B) LoRA token-classification for comparator span BIO tags**"
      ],
      "metadata": {
        "id": "CMuaPSe1SLxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build BIO dataset\n",
        "!pip -q install \"transformers>=4.44.0\" \"peft>=0.13.0\" \"datasets>=2.20.0\" -U\n",
        "\n",
        "import json, ast, re\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (AutoTokenizer, AutoModelForTokenClassification,\n",
        "                          DataCollatorForTokenClassification, Trainer, TrainingArguments)\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "ROOT = Path(\"/mnt/data/cst_v2_joyce_similes\") if Path(\"/mnt/data\").exists() else Path(\"/content/cst_v2_joyce_similes\")\n",
        "PROC = ROOT / \"data\" / \"processed\"\n",
        "CSV_PATH = PROC / \"merged_all_processed.csv\"\n",
        "assert CSV_PATH.exists()\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "df[\"sentence\"] = df[\"sentence\"].astype(str).str.strip()\n",
        "\n",
        "def parse_spans(x):\n",
        "    if isinstance(x, list): return x\n",
        "    if isinstance(x, str):\n",
        "        try:\n",
        "            v = ast.literal_eval(x)\n",
        "            if isinstance(v, list): return v\n",
        "        except Exception: pass\n",
        "    return []\n",
        "\n",
        "df[\"comparator_spans\"] = df[\"comparator_spans\"].apply(parse_spans)\n",
        "if \"story\" not in df.columns:\n",
        "    df[\"story\"] = df.get(\"source_file\",\"ALL\")\n",
        "\n",
        "MODEL_NAME = \"roberta-base\"\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "MAX_LEN = 256\n",
        "label_list = [\"O\",\"B-CMP\",\"I-CMP\"]\n",
        "label2id = {l:i for i,l in enumerate(label_list)}\n",
        "id2label = {i:l for l,i in label2id.items()}\n",
        "\n",
        "def bio_encode(row):\n",
        "    text = row[\"sentence\"]\n",
        "    spans = [(s[\"start\"], s[\"end\"]) for s in row[\"comparator_spans\"] if \"start\" in s and \"end\" in s]\n",
        "    enc = tok(text, max_length=MAX_LEN, truncation=True, return_offsets_mapping=True)\n",
        "    labels = np.zeros(len(enc[\"input_ids\"]), dtype=int)  # all 'O'\n",
        "    for (b,e) in spans:\n",
        "        for i,(o_b,o_e) in enumerate(enc[\"offset_mapping\"]):\n",
        "            if o_b==o_e:  # special tokens\n",
        "                continue\n",
        "            # token overlaps span?\n",
        "            if not (o_e <= b or o_b >= e):\n",
        "                labels[i] = label2id[\"I-CMP\"]\n",
        "        inside = [i for i,(o_b,o_e) in enumerate(enc[\"offset_mapping\"]) if (o_b<e and o_e>b and o_b!=o_e)]\n",
        "        if inside:\n",
        "            labels[inside[0]] = label2id[\"B-CMP\"]\n",
        "    enc.pop(\"offset_mapping\")\n",
        "    enc[\"labels\"] = labels.tolist()\n",
        "    return enc\n",
        "\n",
        "# Train/val split by story\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, val_idx = next(gss.split(df, groups=df[\"story\"]))\n",
        "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
        "val_df   = df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df[[\"sentence\",\"comparator_spans\",\"story\"]])\n",
        "val_ds   = Dataset.from_pandas(val_df[[\"sentence\",\"comparator_spans\",\"story\"]])\n",
        "raw = DatasetDict({\"train\": train_ds, \"validation\": val_ds})\n",
        "tokd = raw.map(bio_encode, remove_columns=raw[\"train\"].column_names)\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tok)\n",
        "\n",
        "# Model + LoRA\n",
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(label_list), id2label=id2label, label2id=label2id)\n",
        "lora_cfg = LoraConfig(task_type=TaskType.TOKEN_CLS, r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "                      target_modules=[\"query\",\"value\",\"key\",\"q_proj\",\"v_proj\",\"k_proj\"])\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=str(ROOT / \"models\" / \"roberta_cmp_bio_lora\"),\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        "    warmup_ratio=0.1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    fp16=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "def compute_token_metrics(p):\n",
        "    # simple token-level F1 for CMP vs O\n",
        "    from sklearn.metrics import f1_score\n",
        "    preds = np.argmax(p.predictions, axis=-1).ravel()\n",
        "    refs  = p.label_ids.ravel()\n",
        "    mask  = refs != -100\n",
        "    preds = preds[mask]; refs = refs[mask]\n",
        "    cmp_idxs = [label2id[\"B-CMP\"], label2id[\"I-CMP\"]]\n",
        "    preds_bin = np.isin(preds, cmp_idxs).astype(int)\n",
        "    refs_bin  = np.isin(refs, cmp_idxs).astype(int)\n",
        "    return {\"cmp_token_f1\": f1_score(refs_bin, preds_bin)}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokd[\"train\"],\n",
        "    eval_dataset=tokd[\"validation\"],\n",
        "    tokenizer=tok,\n",
        "    compute_metrics=compute_token_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(args.output_dir)\n",
        "tok.save_pretrained(args.output_dir)\n",
        "(Path(args.output_dir) / \"bio_labels.json\").write_text(json.dumps(label2id, indent=2))\n",
        "print(\"Saved:\", args.output_dir)\n"
      ],
      "metadata": {
        "id": "S4-gk2jGSQ0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C) Group-stratified evaluation, class weights & calibrated probs**"
      ],
      "metadata": {
        "id": "odsUnoVrSswn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GroupKFold eval with class weights + calibration for the classifier\n",
        "!pip -q install \"scikit-learn>=1.5.0\"\n",
        "\n",
        "import numpy as np, pandas as pd, json, ast, re\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "ROOT = Path(\"/mnt/data/cst_v2_joyce_similes\") if Path(\"/mnt/data\").exists() else Path(\"/content/cst_v2_joyce_similes\")\n",
        "PROC = ROOT / \"data\" / \"processed\"\n",
        "CSV_PATH = PROC / \"comprehensive_linguistic_analysis_corrected-7__processed.csv\"\n",
        "df[\"sentence\"] = df[\"sentence\"].astype(str).str.strip()\n",
        "\n",
        "label_col = next(c for c in [\"gold_category\",\"category\",\"extractor_label\"] if c in df.columns and df[c].dropna().astype(str).nunique()>=2)\n",
        "df[label_col] = df[label_col].astype(str)\n",
        "labels = sorted(df[label_col].unique()) = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "MAX_LEN=256\n",
        "\n",
        "def encode(df_sub):\n",
        "    enc = tok(df_sub[\"sentence\"].tolist(), max_length=MAX_LEN, padding=True, truncation=True)\n",
        "    enc[\"labels\"] = df_sub[\"label_id\"].tolist()\n",
        "    return Dataset.from_dict(enc)\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "reports = []\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(df, y=df[\"label_id\"], groups=df[\"story\"])):\n",
        "    tr, va = df.iloc[tr_idx], df.iloc[va_idx]\n",
        "    ds_tr, ds_va = encode(tr), encode(va)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(labels), id2label=id2label, label2id=label2id)\n",
        "    lora_cfg = LoraConfig(task_type=TaskType.SEQ_CLS, r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "                          target_modules=[\"query\",\"value\",\"key\",\"q_proj\",\"v_proj\",\"k_proj\"])\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=str(ROOT / \"models\" / f\"roberta_cst_lora_cv{fold}\"),\n",
        "        learning_rate=1e-4, per_device_train_batch_size=16, per_device_eval_batch_size=32,\n",
        "        num_train_epochs=3, evaluation_strategy=\"epoch\", save_strategy=\"no\",\n",
        "        logging_steps=100, warmup_ratio=0.1, fp16=torch.cuda.is_available()\n",
        "    )\n",
        "    from torch import nn\n",
        "    class CETrainer(Trainer):\n",
        "        def compute_loss(self, model, inputs, return_outputs=False):\n",
        "            labels = inputs.pop(\"labels\")\n",
        "            outputs = model(**inputs); logits = outputs.logits\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "            return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    trnr = CETrainer(model=model, args=args, train_dataset=ds_tr, eval_dataset=ds_va, tokenizer=tok)\n",
        "    trnr.train()\n",
        "\n",
        "    # evaluate\n",
        "    preds = trnr.predict(ds_va).predictions\n",
        "    yhat = preds.argmax(-1)\n",
        "    rep = classification_report(va[\"label_id\"], yhat, target_names=labels, digits=3)\n",
        "    reports.append(f\"Fold {fold}\\n{rep}\")\n",
        "\n",
        "(Path(ROOT/\"outputs\"/\"groupkfold_reports.txt\")).write_text(\"\\n\\n\".join(reports))\n",
        "print(\"Saved:\", ROOT/\"outputs\"/\"groupkfold_reports.txt\")"
      ],
      "metadata": {
        "id": "3SA2QGaBSvXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit temperature on validation set"
      ],
      "metadata": {
        "id": "8dydIY4BTeBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After training in Section A, reuse: `trainer`, `tok`, `out_dir`, and your label names.\n",
        "\n",
        "import torch, numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "import evaluate, json\n",
        "from pathlib import Path\n",
        "\n",
        "# Grab val logits/labels\n",
        "eval_out = trainer.predict(trainer.eval_dataset)\n",
        "val_logits = torch.tensor(eval_out.predictions)           # [N, C]\n",
        "val_labels = torch.tensor(eval_out.label_ids).long()      # [N]\n",
        "\n",
        "# Temperature scaler\n",
        "class TemperatureScaler(nn.Module):\n",
        "    def __init__(self, init_temp=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = nn.Parameter(torch.ones(1) * float(init_temp))\n",
        "    def forward(self, logits):\n",
        "        # clamp temperature to positive values for stability\n",
        "        temp = torch.clamp(self.temperature, min=1e-6)\n",
        "        return logits / temp\n",
        "\n",
        "def fit_temperature(logits, labels, max_iter=500, lr=1e-2, verbose=True):\n",
        "    logits = logits.detach()\n",
        "    labels = labels.detach()\n",
        "    scaler = TemperatureScaler(1.0)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optim = torch.optim.LBFGS(scaler.parameters(), lr=0.5, max_iter=50) if logits.shape[0] > 2000 else torch.optim.AdamW(scaler.parameters(), lr=lr)\n",
        "\n",
        "    def closure():\n",
        "        optim.zero_grad()\n",
        "        loss = criterion(scaler(logits), labels)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    last = None\n",
        "    if isinstance(optim, torch.optim.LBFGS):\n",
        "        optim.step(closure)\n",
        "        last = closure().item()\n",
        "    else:\n",
        "        for i in range(max_iter):\n",
        "            loss = closure()\n",
        "            optim.step()\n",
        "            last = loss.item()\n",
        "            if verbose and (i % 50 == 0):\n",
        "                print(f\"[fit T] step {i}  NLL={last:.4f}\")\n",
        "    if verbose:\n",
        "        print(\"Fitted T =\", float(torch.clamp(scaler.temperature, min=1e-6).item()), \"final NLL=\", last)\n",
        "    return scaler\n",
        "\n",
        "# Metrics\n",
        "acc = evaluate.load(\"accuracy\")\n",
        "f1  = evaluate.load(\"f1\")\n",
        "\n",
        "def metrics_from_logits(logits, labels):\n",
        "    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
        "    preds = probs.argmax(-1)\n",
        "    return {\n",
        "        \"accuracy\": acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "        \"macro_f1\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
        "    }\n",
        "\n",
        "def expected_calibration_error(probs, labels, n_bins=15):\n",
        "    probs = np.asarray(probs)\n",
        "    conf = probs.max(axis=1)\n",
        "    preds = probs.argmax(axis=1)\n",
        "    labels = np.asarray(labels)\n",
        "    ece = 0.0\n",
        "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
        "    for b0, b1 in zip(bins[:-1], bins[1:]):\n",
        "        mask = (conf > b0) & (conf <= b1)\n",
        "        if not np.any(mask):\n",
        "            continue\n",
        "        acc_bin = (preds[mask] == labels[mask]).mean()\n",
        "        conf_bin = conf[mask].mean()\n",
        "        ece += (mask.mean()) * abs(acc_bin - conf_bin)\n",
        "    return float(ece)\n",
        "\n",
        "# Pre-calibration\n",
        "pre = metrics_from_logits(val_logits, val_labels.numpy())\n",
        "pre_ece = expected_calibration_error(torch.softmax(val_logits, -1).numpy(), val_labels.numpy())\n",
        "print(\"Pre-calibration:\", pre, \"ECE≈\", round(pre_ece, 4))\n",
        "\n",
        "# Fit T\n",
        "scaler = fit_temperature(val_logits, val_labels, lr=1e-2, verbose=True)\n",
        "\n",
        "# Post-calibration\n",
        "post_probs = torch.softmax(scaler(val_logits), dim=-1).numpy()\n",
        "post_preds = post_probs.argmax(-1)\n",
        "post = {\n",
        "    \"accuracy\": acc.compute(predictions=post_preds, references=val_labels.numpy())[\"accuracy\"],\n",
        "    \"macro_f1\": f1.compute(predictions=post_preds, references=val_labels.numpy(), average=\"macro\")[\"f1\"]\n",
        "}\n",
        "post_ece = expected_calibration_error(post_probs, val_labels.numpy())\n",
        "print(\"Post-calibration:\", post, \"ECE≈\", round(post_ece, 4))\n",
        "\n",
        "# Save T with the model\n",
        "T_value = float(torch.clamp(scaler.temperature, min=1e-6).item())\n",
        "calib_path = Path(out_dir) / \"temperature.json\"\n",
        "calib_path.write_text(json.dumps({\"temperature\": T_value}, indent=2))\n",
        "print(\"Saved temperature to:\", calib_path)\n"
      ],
      "metadata": {
        "id": "jOXfdXefTex9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "calibrated model for inference"
      ],
      "metadata": {
        "id": "GApoqI33Tq08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "def load_temperature(model_dir):\n",
        "    p = Path(model_dir) / \"temperature.json\"\n",
        "    if p.exists():\n",
        "        return json.loads(p.read_text()).get(\"temperature\", 1.0)\n",
        "    return 1.0\n",
        "\n",
        "def predict_with_calibration(model_dir, texts, max_len=256, batch_size=32, device=None):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    tok = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
        "    mdl = AutoModelForSequenceClassification.from_pretrained(model_dir).to(device).eval()\n",
        "    T = load_temperature(model_dir)\n",
        "    all_probs, all_preds = [], []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        enc = tok(batch, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = mdl(**enc).logits\n",
        "            logits = logits / max(T, 1e-6)  # apply temperature\n",
        "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
        "            preds = probs.argmax(-1)\n",
        "        all_probs.append(probs); all_preds.append(preds)\n",
        "    return np.vstack(all_probs), np.concatenate(all_preds)"
      ],
      "metadata": {
        "id": "6_hX8LryTmVL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}